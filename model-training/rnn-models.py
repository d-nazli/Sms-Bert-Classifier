# -*- coding: utf-8 -*-
"""rnn-models.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_oG4HsGBcL8kh0SGfFckwFCLVn8LW2N
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# %run "/content/drive/MyDrive/Colab Notebooks/SmsClassification.ipynb"

import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from sklearn.metrics import classification_report
from sklearn.utils.class_weight import compute_class_weight

X_train_text = train_df["masked_text"].tolist()
X_val_text   = val_df["masked_text"].tolist()
X_test_text  = test_df["masked_text"].tolist()

y_train = train_df["group"].values
y_val   = val_df["group"].values
y_test  = test_df["group"].values

MAX_VOCAB = 20000
MAX_LEN = 40

tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token="<OOV>")
tokenizer.fit_on_texts(X_train_text)

X_train = tokenizer.texts_to_sequences(X_train_text)
X_val   = tokenizer.texts_to_sequences(X_val_text)
X_test  = tokenizer.texts_to_sequences(X_test_text)

X_train = pad_sequences(X_train, maxlen=MAX_LEN, padding="post")
X_val   = pad_sequences(X_val,   maxlen=MAX_LEN, padding="post")
X_test  = pad_sequences(X_test,  maxlen=MAX_LEN, padding="post")

classes = np.unique(y_train)

class_weights = compute_class_weight(
    class_weight="balanced",
    classes=classes,
    y=y_train
)

class_weight_dict = dict(zip(classes, class_weights))
print("Class weights:", class_weight_dict)

#lstm model
NUM_CLASSES = 3

model_lstm = Sequential([
    Embedding(
        input_dim=MAX_VOCAB,
        output_dim=128,
        input_length=MAX_LEN
    ),
    LSTM(128),
    Dropout(0.3),
    Dense(NUM_CLASSES, activation="softmax")
])
model_lstm.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model_lstm.summary()

from tensorflow.keras.callbacks import EarlyStopping

early_stop = EarlyStopping(
    monitor="val_loss",
    patience=2,
    restore_best_weights=True
)

history_lstm = model_lstm.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=6,
    batch_size=32,
    class_weight=class_weight_dict,
    callbacks=[early_stop]
)

y_pred_lstm = model_lstm.predict(X_test).argmax(axis=1)

print("LSTM RESULTS")
print(classification_report(y_test, y_pred_lstm))

#bilstm model

model_bilstm = Sequential([
    Embedding(
        input_dim=MAX_VOCAB,
        output_dim=128,
        input_length=MAX_LEN
    ),
    Bidirectional(LSTM(128)),
    Dropout(0.3),
    Dense(NUM_CLASSES, activation="softmax")
])

model_bilstm.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["accuracy"]
)

model_bilstm.summary()

history_bilstm = model_bilstm.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=6,
    batch_size=32,
    class_weight=class_weight_dict,
    callbacks=[early_stop]
)

y_pred_bilstm = model_bilstm.predict(X_test).argmax(axis=1)

print("BiLSTM RESULTS")
print(classification_report(y_test, y_pred_bilstm))
